# issues. As such results have no use in the analysis, the decision was taken to
# remove these features
dataset_check<-dataset_precheck(dataset, parameters, measures)
!is.null(dataset_check)
dataset<-dataset_check$dataset
parameters<-dataset_check$parameters
measures<-dataset_check$measures
range_check<-check_ranges(sample_mins,sample_maxes,parameters)
sample_mins<-cbind(2.0,0.5,0.2,0.0,0.0,0.0)
sample_maxes<-cbind(6,2,2,1,1,1)
range_check<-check_ranges(sample_mins,sample_maxes,parameters)
range_check
parameters
PARAMETERS
parameters<-c("quantity","home_source_distance","initial_partition_length","alpha","epsilon","memory_factor")
range_check<-check_ranges(sample_mins,sample_maxes,parameters)
range_check
sample_mins<-range_check$sample_mins
sample_maxes<-range_check$sample_maxes
normalise=TRUE
if (normalise == TRUE) {
if (is.null(sample_mins) | is.null(sample_maxes) | is.null(parameters))
message("You need to specify sampling mins and maxes for each parameter,
and parameter names, for correct normalisation. Terminated.")
else {
normed_data <- normalise_dataset(dataset, sample_mins, sample_maxes,
parameters)
dataset <- normed_data$scaled
pre_normed_data_mins <- normed_data$mins
pre_normed_data_maxes <- normed_data$maxs
}
}
positions <- sample(nrow(dataset), size = floor( (nrow(dataset) / 100)
* percent_train))
training <- dataset[positions, ]
remainder <- dataset[ -positions, ]
testing_positions <- sample(
nrow(remainder), size = floor(
(nrow(remainder) / 100) * ( (percent_test / (percent_test +
percent_validation))
* 100)))
testing <- remainder[testing_positions, ]
validation <- remainder[-testing_positions, ]
partitioned_data <- list("training" = training, "testing" = testing,
"validation" = validation,
"pre_normed_mins" = pre_normed_data_mins,
"pre_normed_maxes" = pre_normed_data_maxes,
"parameters"=parameters,
"measures"=measures)
percent_train = 75
percent_test = 15
percent_validation = 10
if (normalise == TRUE) {
if (is.null(sample_mins) | is.null(sample_maxes) | is.null(parameters))
message("You need to specify sampling mins and maxes for each parameter,
and parameter names, for correct normalisation. Terminated.")
else {
normed_data <- normalise_dataset(dataset, sample_mins, sample_maxes,
parameters)
dataset <- normed_data$scaled
pre_normed_data_mins <- normed_data$mins
pre_normed_data_maxes <- normed_data$maxs
}
}
positions <- sample(nrow(dataset), size = floor( (nrow(dataset) / 100)
* percent_train))
training <- dataset[positions, ]
remainder <- dataset[ -positions, ]
testing_positions <- sample(
nrow(remainder), size = floor(
(nrow(remainder) / 100) * ( (percent_test / (percent_test +
percent_validation))
* 100)))
testing <- remainder[testing_positions, ]
validation <- remainder[-testing_positions, ]
partitioned_data <- list("training" = training, "testing" = testing,
"validation" = validation,
"pre_normed_mins" = pre_normed_data_mins,
"pre_normed_maxes" = pre_normed_data_maxes,
"parameters"=parameters,
"measures"=measures)
devtools::build()
devtools::load_all(".")
devtools::build()
MEASURES
devtools::load_all(".")
devtools::build()
model_list<-c("SVM","GLM")
parameters
measures
partitioned_data<-partition_dataset(dataset,parameters,measures,normalise=TRUE,sample_mins = rbind(2,0.5,0.2,0,0,0),sample_maxes=rbind(6,2,2,1,1,1))
head(dataset)
parameters
measures
MEASURES
measures<-MEASURES
partitioned_data<-partition_dataset(dataset,parameters,measures,normalise=TRUE,sample_mins = rbind(2,0.5,0.2,0,0,0),sample_maxes=rbind(6,2,2,1,1,1))
partitioned_data<-partition_dataset(dataset,parameters,measures,normalise=TRUE,sample_mins = rbind(c(2,0.5,0.2,0,0,0)),sample_maxes=rbind(c(6,2,2,1,1,1)))
algorithm_settings<-NULL
normalised<-TRUE
# Generate_requested_emulations checks for algorithm_settings
emulators_with_test_preds <- generate_requested_emulations(
model_list, partitioned_data, parameters, measures, algorithm_settings,
timepoint, normalised, output_formats)
parameters
measures
partitioned_data$training
# Generate_requested_emulations checks for algorithm_settings
emulators_with_test_preds <- generate_requested_emulations(
model_list, partitioned_data, parameters, measures, algorithm_settings,
timepoint, normalised, output_formats)
parameters
measures
devtools::load_all(".")
partitioned_data<-partition_dataset(dataset,parameters,measures,normalise=TRUE,sample_mins = rbind(c(2,0.5,0.2,0,0,0)),sample_maxes=rbind(c(6,2,2,1,1,1)))
names(partitioned_data)
partitioned_data$parameters
partitioned_data$measures
devtools::load_all(".")
head(dataset)
dataset<-read.csv("/home/kja505/Downloads/LHCcombinedParamsAndResults.csv",header=T)
partitioned_data<-partition_dataset(dataset,parameters,measures,normalise=TRUE,sample_mins = rbind(c(2,0.5,0.2,0,0,0)),sample_maxes=rbind(c(6,2,2,1,1,1)))
# Generate_requested_emulations checks for algorithm_settings
emulators_with_test_preds <- generate_requested_emulations(
model_list, partitioned_data, parameters, measures, algorithm_settings,
timepoint, normalised, output_formats)
output_formats=c("pdf")
# Generate_requested_emulations checks for algorithm_settings
emulators_with_test_preds <- generate_requested_emulations(
model_list, partitioned_data, parameters, measures, algorithm_settings,
timepoint, normalised, output_formats)
timepoint<-NULL
# Generate_requested_emulations checks for algorithm_settings
emulators_with_test_preds <- generate_requested_emulations(
model_list, partitioned_data, parameters, measures, algorithm_settings,
timepoint, normalised, output_formats)
!(typeof(emulators_with_test_preds) == "list")
emulators <- emulators_with_test_preds$emulators
generated_ensemble <- create_ensemble(
emulators, all_model_predictions, partitioned_data$testing, measures,
model_list, emulators_with_test_preds$pre_normed_mins,
emulators_with_test_preds$pre_normed_maxes, algorithm_settings = NULL,
normalise = normalised, timepoint = NULL, output_formats)
all_model_predictions <- emulators_with_test_preds$prediction_set
emulators <- emulators_with_test_preds$emulators
generated_ensemble <- create_ensemble(
emulators, all_model_predictions, partitioned_data$testing, measures,
model_list, emulators_with_test_preds$pre_normed_mins,
emulators_with_test_preds$pre_normed_maxes, algorithm_settings = NULL,
normalise = normalised, timepoint = NULL, output_formats)
names(emulators_with_test_preds)
devtools::load_all(".")
parameters
partitioned_data$parameters
partitioned_data$parameters==parameters
measures==partitioned_data$measures
measures
partitioned_data$measures
devtools::load_all(".")
if(length(parameters) > partitioned_data$parameters)
parameters<-partitioned_data$parameters
if(length(measures) > partitioned_data$measures)
measures<-partitioned_data$measures
if(length(parameters) > length(partitioned_data$parameters))
parameters<-partitioned_data$parameters
if(length(measures) > length(partitioned_data$measures))
measures<-partitioned_data$measures
all_model_predictions <- emulators_with_test_preds$prediction_set
emulators <- emulators_with_test_preds$emulators
generated_ensemble <- create_ensemble(
emulators, all_model_predictions, partitioned_data$testing, measures,
model_list, emulators_with_test_preds$pre_normed_mins,
emulators_with_test_preds$pre_normed_maxes, algorithm_settings = NULL,
normalise = normalised, timepoint = NULL, output_formats)
parameters
measures
ensemble_emulations<-emulators
all_model_predictions<-all_emulator_predictions
all_emulator_predictions<-all_model_predictions
emulator_test_data<-partitioned_data$testing
measures
emulator_types<-model_list
pre_normed_mins<-emulators_with_test_preds$pre_normed_mins
pre_normed_maxes<-emulators_with_test_preds$pre_normed_maxes
algorithm_settings<-NULL
normalise<-normalised
algorithm_settings <- emulation_algorithm_settings()
# Calculate the weightings
weights <- calculate_weights_for_ensemble_model(
all_emulator_predictions, emulator_test_data, measures, emulator_types,
algorithm_settings$num_of_generations)
model_weights<-weights
all_model_predictions<-all_emulator_predictions
num_generations<-800000
all_ensemble_predictions <- NULL
m<-1
#As all the measure data is in the same dataset, we need to refer to the
# correct columns (always the same measure). Ensure this using a sequence
columns <- seq(m, ncol(all_model_predictions), by = length(measures))
columns
# Extract this data for training the net
predicted_response <- all_model_predictions[, columns]
# Extract the weights
measure_weights <- model_weights[m, ]
model_weights
weights()
weights
# Calculate the weightings
weights <- calculate_weights_for_ensemble_model(
all_emulator_predictions, emulator_test_data, measures, emulator_types,
algorithm_settings$num_of_generations)
all_emulator_predictions<-all_model_predictions
emulator_types
weights <- NULL
# As all the measure data is in the same dataset, we need to refer to the
# correct columns (always the same measure). Can ensure we do this using
# a sequence
columns <- seq(m, ncol(all_model_predictions), by = length(measures))
# Extract this data for training the net
weightings_net_training_data <- all_model_predictions[, columns]
model_formula <- generate_model_formula(
colnames(weightings_net_training_data), measures[m])
weightings_net_training_data
columns
measures
m
length(measures)
head(all_model_predictions)
# Extract this data for training the net
weightings_net_training_data <- all_model_predictions[, columns]
weightings_net_training_data
colnames(weightings_net_training_data)
names(weightings_net_training_data)
model_formula <- generate_model_formula(
names(weightings_net_training_data), measures[m])
model_formula
# Need to add the actual result to the training data, so weights can be
# appropriately set
weightings_net_training_data <- cbind(weightings_net_training_data,
emulator_test_data[, measures[m]])
# Set the column header for the measure - needed for the formula
# to work correctly
colnames(weightings_net_training_data)[ncol(
weightings_net_training_data)] <- measures[m]
# Generate the neural network
ensemble_nn <- neuralnet::neuralnet(model_formula, data =
weightings_net_training_data,
hidden = c(0),
linear.output = T,
stepmax = num_of_generations)
devtools::load_all(".")
devtools::load()
devtools::load_all()
devtools::build()
# Calculate the weightings
weights <- calculate_weights_for_ensemble_model(
all_emulator_predictions, emulator_test_data, measures, emulator_types,
algorithm_settings$num_of_generations)
weights <- NULL
m<-1
# As all the measure data is in the same dataset, we need to refer to the
# correct columns (always the same measure). Can ensure we do this using
# a sequence
columns <- seq(m, ncol(all_model_predictions), by = length(measures))
# Extract this data for training the net
weightings_net_training_data <- all_model_predictions[, columns]
weightings_net_training_data
model_formula <- generate_model_formula(
names(weightings_net_training_data), measures[m])
# Need to add the actual result to the training data, so weights can be
# appropriately set
weightings_net_training_data <- cbind(weightings_net_training_data,
emulator_test_data[, measures[m]])
weightings_net_training_data
weightings_net_training_data)] <- measures[m]
weightings_net_training_data)] <- measures[m]
colnames(weightings_net_training_data)
[ncol(
weightings_net_training_data)]
ncol(
weightings_net_training_data)
colnames(weightings_net_training_data)[ncol(
weightings_net_training_data)]
weightings_net_training_data
# Extract this data for training the net
weightings_net_training_data <- all_model_predictions[, columns]
weightings_net_training_data
ncol(weightings_net_training_data)
# Extract this data for training the net
weightings_net_training_data <- cbind(all_model_predictions[, columns])
# Extract this data for training the net
weightings_net_training_data
# Extract this data for training the net
weightings_net_training_data <- rbind(all_model_predictions[, columns])
# Extract this data for training the net
weightings_net_training_data
# Extract this data for training the net
ncol(weightings_net_training_data)
model_formula <- generate_model_formula(
names(weightings_net_training_data), measures[m])
model_formula <- generate_model_formula(
colnames(weightings_net_training_data), measures[m])
model_formula
# Need to add the actual result to the training data, so weights can be
# appropriately set
weightings_net_training_data <- cbind(weightings_net_training_data,
emulator_test_data[, measures[m]])
# Set the column header for the measure - needed for the formula
# to work correctly
colnames(weightings_net_training_data)[ncol(
weightings_net_training_data)] <- measures[m]
# Generate the neural network
ensemble_nn <- neuralnet::neuralnet(model_formula, data =
weightings_net_training_data,
hidden = c(0),
linear.output = T,
stepmax = num_of_generations)
num_of_generations = 800000
# Generate the neural network
ensemble_nn <- neuralnet::neuralnet(model_formula, data =
weightings_net_training_data,
hidden = c(0),
linear.output = T,
stepmax = num_of_generations)
# Get the weights to apply to all the models, on from 2 as 1 is the bias
model_weights <- ensemble_nn$weights[[1]][[1]][2:ncol(
weightings_net_training_data + 1)]
weights <- rbind(weights, model_weights)
devtools::load_all(".")
devtools::build9)
devtools::build()
load("~/Desktop/experiment_results.Rda")
parameters<-c("stableBindProbability","chemokineExpressionThreshold","initialChemokineExpressionValue","maxChemokineExpressionValue","maxProbabilityOfAdhesion","adhesionFactorExpressionSlope")
minvals <- c(10, 0.10, 0.10, 0.015, 0.1, 0.25)
maxvals <- c(100, 0.9, 0.50, 0.08, 0.95, 5.0)
number_samples<-65
number_curves<-3
measures<-c("Velocity","Displacement")
percent_train = 75
percent_test = 15
percent_validation = 10
normalise=TRUE
sample_mins<-apply(experiment_results,2,min)
sample_maxes<-apply(experiment_results,2,max)
check_data_partitions(percent_train, percent_test, percent_validation)
devtools::load_all(".")
check_data_partitions(percent_train, percent_test, percent_validation)
!is.null(nrow(dataset)) && nrow(dataset)>0
# Added 18th October 2018 in developing RoboSpartan - was noticed that user could
# submit dataset where whole columns were the same, which in normalisation causes
# issues. As such results have no use in the analysis, the decision was taken to
# remove these features
dataset_check<-dataset_precheck(dataset, parameters, measures)
parameters
measures
dataset<-experiment_results
# Added 18th October 2018 in developing RoboSpartan - was noticed that user could
# submit dataset where whole columns were the same, which in normalisation causes
# issues. As such results have no use in the analysis, the decision was taken to
# remove these features
dataset_check<-dataset_precheck(dataset, parameters, measures)
!is.null(dataset_check)
range_check<-check_ranges(sample_mins,sample_maxes,parameters)
sample_mins<-range_check$sample_mins
sample_maxes<-range_check$sample_maxes
sample_mins
sample_maxes<-apply(experiment_results,2,max)
sample_mins<-apply(experiment_results,2,min)
length(sample_mins)
length(parameters)
length(sample_mins)==length(parameters) & length(sample_maxes)==length(parameters)
length(sample_maxes)==length(parameters)
sample_maxezx
sample_maxes
colnames(sample_maxes)
names(sample_maxes)
# If we normalise, we need to have the mins and maxes for parameters and
# measures for denormalisation of results. If we don't normalise then
# there will be no denormalisation, but the values being passed will not
# be initialised, so we need to cope with both here
pre_normed_data_mins <- NULL
pre_normed_data_maxes <- NULL
normed_data <- normalise_dataset(dataset, sample_mins, sample_maxes,
parameters)
normed_data
names(sample_mins)
sampleMins <-cbind(0,0.1,0.1,0.015,0.1,0.25)
sampleMins
names(sampleMins)
colnames(sampleMins)
names(sampleMins)<-c(4,5,6,7,8,9)
colnames(sampleMins)
devtools::load_all(".")
devtools::build()
head(experiment_results)
# Added 18th October 2018 in developing RoboSpartan - was noticed that user could
# submit dataset where whole columns were the same, which in normalisation causes
# issues. As such results have no use in the analysis, the decision was taken to
# remove these features
dataset_check<-dataset_precheck(dataset, parameters, measures)
dataset_check
is.null(colnames(sample_mins)) | is.null(colnames(sample_maxes))
is.null(colnames(sample_mins))
sample_mins
names(sample_mins)
is.null(names(sample_mins)) | is.null(names(sample_maxes))
devtools::load_all()
devtools::build()
load("/home/kja505/Documents/spartanDB/Built_Emulation_RF_SVM.Rda")
View(built_emulation)
two_emulators<-built_emulation
load("/home/kja505/Documents/spartanDB/Built_Emulation_RF.Rda")
one_emulator<-built_emulation
names(two_emulators)
names(one_emulator)
names(two_emulators$emulators
)
two_emulators$emulators
one_emulator$emulators
emulation<-two_emulators
load("~/Desktop/validationSet.Rda")
data_to_predict<-validation_set
normalise_result<-FALSE
normalise=FALSE
normalise_result<-TRUE
length(emulation$emulators)==1
predictions <- NULL
length(emulation$emulators)
e<-1
model_predictions <- generate_predictions_from_emulator(
emulation$emulators[[e]], parameters, measures, data_to_predict)
model_predictions
predictions <- cbind(
predictions, extract_predictions_from_result_list(
model_predictions, emulation$emulators[[e]]$type, measures))
predictions
e<-2
model_predictions <- generate_predictions_from_emulator(
emulation$emulators[[e]], parameters, measures, data_to_predict)
predictions <- cbind(
predictions, extract_predictions_from_result_list(
model_predictions, emulation$emulators[[e]]$type, measures))
predictions
# rbind used to get the mins and maxes into the correct format
# (compatible with all other calls to this function)
predictions <- denormalise_dataset(
predictions, rbind(emulation$pre_normed_mins[measures]),
rbind(emulation$pre_normed_maxes[measures] ))
predictions
predictions <- NULL
# Run this for all emulators in the object
for (e in 1:length(emulation$emulators)) {
model_predictions <- generate_predictions_from_emulator(
emulation$emulators[[e]], parameters, measures, data_to_predict)
predictions <- cbind(
predictions, extract_predictions_from_result_list(
model_predictions, emulation$emulators[[e]]$type, measures))
}
normalised_data<-predictions
scaled_mins<-rbind(emulation$pre_normed_mins[measures])
scaled_maxes<-rbind(emulation$pre_normed_maxes[measures] )
scaled_mins
scaled_maxes
length(emulation$emulators))
length(emulation$emulators)
predictions
emulation$emulators$type
emulation$emulators
emulation$emulators[[1]]
emulation$emulators[[1]]$type
paste0(emulation$emulators[[1]]$type,"_",measures)
# Generate the header of the predictions for this emulator so these can be extracted: emulator type "_" measure
predictions_header<-paste0(emulation$emulators[[1]]$type,"_",measures)
predictions[predictions_header]
predictions[,predictions_header]
predictions
i<-1
# Generate the header of the predictions for this emulator so these can be extracted: emulator type "_" measure
predictions_header<-paste0(emulation$emulators[[i]]$type,"_",measures)
predictions[,predictions_header] <- denormalise_dataset(
predictions[,predictions_header], rbind(emulation$pre_normed_mins[measures]),
rbind(emulation$pre_normed_maxes[measures]))
predictions
i<-2
predictions[,predictions_header] <- denormalise_dataset(
predictions[,predictions_header], rbind(emulation$pre_normed_mins[measures]),
rbind(emulation$pre_normed_maxes[measures]))
predictions
predictions <- NULL
# Run this for all emulators in the object
for (e in 1:length(emulation$emulators)) {
model_predictions <- generate_predictions_from_emulator(
emulation$emulators[[e]], parameters, measures, data_to_predict)
predictions <- cbind(
predictions, extract_predictions_from_result_list(
model_predictions, emulation$emulators[[e]]$type, measures))
}
for(i in 1:length(emulation$emulators))
{
# Generate the header of the predictions for this emulator so these can be extracted: emulator type "_" measure
predictions_header<-paste0(emulation$emulators[[i]]$type,"_",measures)
predictions[,predictions_header] <- denormalise_dataset(
predictions[,predictions_header], rbind(emulation$pre_normed_mins[measures]),
rbind(emulation$pre_normed_maxes[measures]))
}
predictions
devtools::load_all(".")
devtools::build()
devtools::load_all(".")
devtools::build()
devtools::load_all(".")
devtools::build()
