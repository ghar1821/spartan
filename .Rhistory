MEASURES<-measures
LHCSUMMARYFILENAME<-"Emulated_LHC_Summary.csv"
CORCOEFFSOUTPUTFILE<-"CorrelationCoefficients.csv"
lhc_result_file <- read_from_csv(file.path(FILEPATH,LHCSUMMARYFILENAME))
devtools::load_all(".")
devtools::load_all(".")
# Analyse:
# Method takes care of adding timepoint to the file names
prccs<-lhc_generatePRCoEffs(FILEPATH=filepath, parameters,  measures,
LHCSUMMARYFILENAME=NULL,
"CorrelationCoefficients.csv",
c(timepoint), timepointscale, check_done=TRUE,
write_csv_files=write_csv_files,
lhc_summary_object=param_values_with_predictions)
prccs
devtools::load_all(".")
devtools::load_all(".")
lhc_graphMeasuresForParameterChange(
filepath, parameters, measures, measure_scale, CORCOEFFSOUTPUTFILE=NULL,
LHCSUMMARYFILENAME=NULL, OUTPUT_TYPE = c("PNG"), TIMEPOINTS = c(timepoint),
TIMEPOINTSCALE = timepointscale, check_done=TRUE, corcoeffs_output_object=prccs,
lhc_summary_object=param_values_with_predictions)
devtools::build()
devtools::load_all(".")
devtools::build()
devtools::load_all(".")
devtools::build()
devtools::load_all(".")
devtools::build()
load("/home/kja505/Desktop/partitionedData.Rda")
emulator_list<-c("RF","SVM")
model_list<-emulator_list
parameters
measures
algorithm_settings<-NULL
algorithm_settings <- emulation_algorithm_settings()
length(parameters) != length(partitioned_data$parameters)
length(measures) != length(partitioned_data$measures)
all_model_predictions <- NULL
all_model_accuracy_stats <- NULL
# Structure to store the emulators created
ensemble_emulators <- vector("list", length(model_list))
model_index<-1
# Sse the same code to generate all the currently acceptable model types
model_fit <- generate_emulator_model(model_list[model_index],
parameters, measures,
partitioned_data,
algorithm_settings,
timepoint = timepoint, normalised,
output_formats)
technique<-model_list[model_index]
dataset<-partitioned_data
start.time <- proc.time()
models_for_all_measures <- vector("list", length(measures))
m<-1
# Make the formula
model_formula <- generate_model_formula(parameters, measures[m])
print(model_formula)
technique
# Generate the RF:
model_fit <- randomForest::randomForest(
model_formula, data = dataset$training[, c(parameters, measures[m])],
ntree = algorithm_settings$num_trees)
# Stats for the training fit:
model_training_fit <- predict(model_fit,
newdata = dataset$training[, parameters])
algorithm_settings$plot_test_accuracy == TRUE
normalised
# Want to denormalise the predictions such that these are plotted
# on the correct scale
unscaled_predictions <- denormalise_dataset(
cbind(model_training_fit),
rbind(dataset$pre_normed_mins[measures[m]]),
rbind(dataset$pre_normed_maxes[measures[m]]))
unscaled_simulations <- denormalise_dataset(
dataset$training, rbind(dataset$pre_normed_mins),
rbind(dataset$pre_normed_maxes))
normalised_data<-dataset$training
scaled_mins<-rbind(dataset$pre_normed_mins)
scaled_maxes<-rbind(dataset$pre_normed_maxes)
ncol(normalised_data)
scaled_mins
scaled_maxes
c<-1
normalised_data[, c]
normalised_data[, c] <- (normalised_data[, c] *
(scaled_maxes[c] -
scaled_mins[c])) + scaled_mins[c]
unscaled_simulations <- denormalise_dataset(
dataset$training, rbind(dataset$pre_normed_mins),
rbind(dataset$pre_normed_maxes))
devtools::load_all(".")
unscaled_simulations <- denormalise_dataset(
dataset$training, rbind(dataset$pre_normed_mins),
rbind(dataset$pre_normed_maxes))
normalised_data[,2]
normalised_data
partitioned_data
partitioned_data$training
partitioned_data$training[,c]
normalised_data
partitioned_data
partitioned_data$training
normalised_data<-dataset$training
normalised_data
ncol(normalised_data)
for (c in 1:ncol(normalised_data)) {
print(c)
normalised_data[, c] <- (normalised_data[, c] *
(scaled_maxes[c] -
scaled_mins[c])) + scaled_mins[c]
}
scaled_maxes[c]
scaled_mins[c]
scaled_maxes[c] -
scaled_mins[c]
normalised_data[, c]
c
normalised_data
normalised_data<-dataset$training
normalised_data[,1]
normalised_data[,2]
normalised_data[,3]
normalised_data[,4]
normalised_data[,5]
normalised_data[,6]
normalised_data[,7]
normalised_data[,8]
normalised_data[,9]
(normalised_data[, c] *
(scaled_maxes[c] -
scaled_mins[c])) + scaled_mins[c]
normalised_data[, c]
(scaled_maxes[c] -
scaled_mins[c])) + scaled_mins[c]
(normalised_data[, c] *
(scaled_maxes[c] -
scaled_mins[c])) + scaled_mins[c]
normalised_data<-data.frame(dataset$training)
(normalised_data[, c] *
(scaled_maxes[c] -
scaled_mins[c])) + scaled_mins[c]
scaled_mins
rbind(scaled_mins)
ncol(scaled_mins)
f<-data.frame(scaled_mins)
g<-data.frame(scaled_maxes)
normalised_data[,c]
normalised_data[,c]*scaled_maxes[c]
normalised_data[,c]*scaled_maxes[,c]
normalised_data[, c]
normalised_data
normalised_data<-data.frame(normalised_data)
normalised_data[,c]
scaled_maxes[c]
scaled_maxes[c][[1]]
(normalised_data[, c] *
(scaled_maxes[c][[1]] -
scaled_mins[c][[1]])) + scaled_mins[c][[1]]
for (c in 1:ncol(normalised_data)) {
print(c)
normalised_data[, c] <- (normalised_data[, c] *
(scaled_maxes[c][[1]] -
scaled_mins[c][[1]])) + scaled_mins[c][[1]]
}
normalised_data
devtools::load_all()
devtools::build()
devtools::test()
data("emulated_lhc_values")
download.file("www.kieranalden.info/spartan/test_data/built_ensemble_72.Rda", "built_ensemble")
# Load this in
load("built_ensemble")
dir.create(file.path(getwd(),"SA"))
emulate_lhc_sampled_parameters(file.path(getwd(),"SA"), built_ensemble,
c("stableBindProbability","chemokineExpressionThreshold",
"initialChemokineExpressionValue",
"maxChemokineExpressionValue","maxProbabilityOfAdhesion",
"adhesionFactorExpressionSlope"), c("Velocity","Displacement"),
c("microns","microns/min"), dataset = emulated_lhc_values, normalise = TRUE)
filepath<-file.path(getwd(),"SA")
surrogateModel<-built_ensemble
parameters
measures
measure_scale
dataset<-emulated_lhc_values
normalise = TRUE
spartan_sample <- dataset
param_values_with_predictions <- NULL
for (samp in 1:nrow(spartan_sample)) {
# Retrieve the parameter row:
params <- spartan_sample[samp, ]
# generate the prediction - need to be careful here whether we are
# using an ensemble or emulator
if(ensemble_set == FALSE) {
prediction <- emulator_predictions(surrogateModel, parameters, measures,
params, normalise)
} else {
prediction <- use_ensemble_to_generate_predictions(surrogateModel,
params, parameters,
measures,
normalise_values = normalise,
normalise_result = normalise)
}
# Bind to the list of parameters
param_values_with_predictions <- rbind(param_values_with_predictions,
cbind(params, prediction))
}
# Now write this out as if it was the spartan LHC summary
colnames(param_values_with_predictions) <- c(parameters, measures)
write_csv_files = TRUE
lhcsummaryfilename <- file.path(filepath, "Emulated_LHC_Summary.csv")
correlation_coeffs <- file.path(filepath, "CorrelationCoefficients.csv")
write.csv(param_values_with_predictions, lhcsummaryfilename,
quote = FALSE, row.names = FALSE)
# Analyse:
# Method takes care of adding timepoint to the file names
prccs<-lhc_generatePRCoEffs(FILEPATH=filepath, parameters,  measures,
LHCSUMMARYFILENAME=NULL,
"CorrelationCoefficients.csv",
c(timepoint), timepointscale, check_done=TRUE,
write_csv_files=write_csv_files,
lhc_summary_object=param_values_with_predictions)
graph_results
graph_results=TRUE
lhc_graphMeasuresForParameterChange(
filepath, parameters, measures, measure_scale, CORCOEFFSOUTPUTFILE=NULL,
LHCSUMMARYFILENAME=NULL, OUTPUT_TYPE = c("PNG"), TIMEPOINTS = c(timepoint),
TIMEPOINTSCALE = timepointscale, check_done=TRUE, corcoeffs_output_object=prccs,
lhc_summary_object=param_values_with_predictions)
FILEPATH<-filepath
PARAMETERS<-parameters
MEASURES<-measures
MEASURE_SCALE<-measure_scale
CORCOEFFSOUTPUTFILE=NULL
LHCSUMMARYFILENAME=NULL
OUTPUT_TYPE = c("PNG")
corcoeffs_output_object=prccs
lhc_summary_object=param_values_with_predictions
!is.null(CORCOEFFSOUTPUTFILE)
corcoeffs <- corcoeffs_output_object
lhcresult <- lhc_summary_object
message ("Generating output graphs for LHC Parameter Analysis")
p<-1
m<-1
corr_result <- corcoeffs[
p, paste(MEASURES[m], "_Estimate", sep = "")]
corr_result
corcoeffs
corcoeffs_output_object
prccs
param_values_with_predictions
ILEPATH=filepath
FILEPATH=filepath
LHCSUMMARYFILENAME=NULL
"CorrelationCoefficients.csv"
CORCOEFFSOUTPUTFILE<-"CorrelationCoefficients.csv"
cor_calc_method=c("s")
lhc_summary_object=param_values_with_predictions
!is.null(LHCSUMMARYFILENAME)
!is.null(lhc_summary_object)
lhc_result_file<-lhc_summary_object
message("Generating Partial Rank Correlation Coefficients (lhc_generatePRCoEffs)")
COEFFRESULTS <- calculate_prccs_all_parameters(PARAMETERS, lhc_result_file,
MEASURES, cor_calc_method)
COEFFRESULTS
write_csv_files
corcoeffs <- corcoeffs_output_object
corcoeffs
prccs
devtools::load_all(".")
emulate_lhc_sampled_parameters(file.path(getwd(),"SA"), built_ensemble,
c("stableBindProbability","chemokineExpressionThreshold",
"initialChemokineExpressionValue",
"maxChemokineExpressionValue","maxProbabilityOfAdhesion",
"adhesionFactorExpressionSlope"), c("Velocity","Displacement"),
c("microns","microns/min"), dataset = emulated_lhc_values, normalise = TRUE,
write_csv_files = TRUE)
devtools::load_all(".")
emulate_lhc_sampled_parameters(file.path(getwd(),"SA"), built_ensemble,
c("stableBindProbability","chemokineExpressionThreshold",
"initialChemokineExpressionValue",
"maxChemokineExpressionValue","maxProbabilityOfAdhesion",
"adhesionFactorExpressionSlope"), c("Velocity","Displacement"),
c("microns","microns/min"), dataset = emulated_lhc_values, normalise = TRUE,
write_csv_files = FALSE)
predictions<-emulate_lhc_sampled_parameters(file.path(getwd(),"SA"), built_ensemble,
c("stableBindProbability","chemokineExpressionThreshold",
"initialChemokineExpressionValue",
"maxChemokineExpressionValue","maxProbabilityOfAdhesion",
"adhesionFactorExpressionSlope"), c("Velocity","Displacement"),
c("microns","microns/min"), dataset = emulated_lhc_values, normalise = TRUE,
write_csv_files = FALSE)
file.exists(file.path(getwd(),"SA","stableBindProbability_Velocity.png"))
devtools::load_all(".")
devtools::test()
devtools::check()
devtools::load_all(".")
library(spartan)
# Folder containing the example simulation results. Make sure the folder is unzipped
FILEPATH <- "/home/kja505/Downloads/SwarmTaxis/Results/Robustness_Analysis"
# Array of the parameters to be analysed.
# Note only two of the six here for download size reasons
PARAMETERS <- c("quantity")
# Similar to the sampling function discussed above, there are two ways to specify
# parameter value information in the analysis. Ensure you are using the appropriate
# method, setting these to NULL if using the alternative (see comments in sampling
# function description).
# Method 1:
PMIN <- c(2)
PMAX <- c(28)
PINC <- c(1)
BASELINE <- c(25)
MEASURES <- c("distanceToBeacon","robotDensity")
# What each measure represents. Used in graphing results
MEASURE_SCALE <- c("Distance", NULL)
CSV_FILE_NAME <- "Robustness_Summary.csv"
# The results of the A-Test comparisons of each parameter value against that of the
# parameters baseline value are output as a file. This sets the name of this file.
# Current versions of spartan output this to a CSV file
ATESTRESULTSFILENAME <- "Omega_ATests.csv"
# A-Test result value either side of 0.5 at which the difference between two sets of
# results is significant
ATESTSIGLEVEL <- 0.23
result <- read_from_csv(file.path(FILEPATH, CSV_FILE_NAME))
head(result)
# Firstly filter when the simulation results were at baseline values
baseline_result <- subset_results_by_param_value_set(PARAMETERS, result,
BASELINE)
baseline_result
BASELINE
RESULT_SET<-result
PARAMETER_VALUES_TO_FILTER_BY<-BASELINE
# TAKE A COPY OF THE RESULT SET
PARAM_RESULT <- RESULT_SET
P<-1
PARAM_RESULT[[PARAMETERS[P]]]
as.numeric(PARAMETER_VALUES_TO_FILTER_BY[P])
nrow(result)
result <- read_from_csv(file.path(FILEPATH, CSV_FILE_NAME))
nrow(result)
head(result)
# TAKE A COPY OF THE RESULT SET
PARAM_RESULT <- RESULT_SET
RESULT_SET<-result
# TAKE A COPY OF THE RESULT SET
PARAM_RESULT <- RESULT_SET
nrow(PARAM_RESULT)
PARAM_RESULT[[PARAMETERS[P]]]
tail(PARAM_RESULT[[PARAMETERS[P]]])
# RoboSpartan had issues with checking the filepath existed, so for the moment this check
# has been removed
#if (file.exists(FILEPATH)) {
message("Plotting result distribution for each parameter (oat_plotResultDistribution)")
# Folder containing the example simulation results. Make sure the folder is unzipped
FILEPATH <- "/home/kja505/Downloads/SwarmTaxis/Results/Robustness_Analysis"
# Array of the parameters to be analysed.
# Note only two of the six here for download size reasons
PARAMETERS <- c("quantity")
# Similar to the sampling function discussed above, there are two ways to specify
# parameter value information in the analysis. Ensure you are using the appropriate
# method, setting these to NULL if using the alternative (see comments in sampling
# function description).
# Method 1:
PMIN <- c(2)
PMAX <- c(28)
PINC <- c(1)
BASELINE <- c(25)
MEASURES <- c("distanceToBeacon","robotDensity")
# What each measure represents. Used in graphing results
MEASURE_SCALE <- c("Distance", NULL)
CSV_FILE_NAME <- "Robustness_Summary.csv"
# The results of the A-Test comparisons of each parameter value against that of the
# parameters baseline value are output as a file. This sets the name of this file.
# Current versions of spartan output this to a CSV file
ATESTRESULTSFILENAME <- "Omega_ATests.csv"
# A-Test result value either side of 0.5 at which the difference between two sets of
# results is significant
ATESTSIGLEVEL <- 0.23
# RoboSpartan had issues with checking the filepath existed, so for the moment this check
# has been removed
#if (file.exists(FILEPATH)) {
message("Plotting result distribution for each parameter (oat_plotResultDistribution)")
# NEW TO SPARTAN VERSION 2
# READS SIMULATION RESPONSES FROM A CSV FILE, IN THE FORMAT: PARAMETER
# VALUES (COLUMNS), SIMULATION OUTPUT MEASURES IN A CHANGE TO SPARTAN 1,
# THE FIRST FUNCTION THAT PROCESSES SIMULATION RESPONSES CREATES THIS
# FILE, NOT MEDIANS FOR EACH PARAMETER AS IT USED TO. THIS WAY WE ARE
# NOT DEALING WITH TWO METHODS OF SIMULATION RESULT SPECIFICATION
# READ IN THE OAT RESULT FILE
RESULT <- read.csv(make_path(c(FILEPATH, CSV_FILE_NAME)), sep = ",",
header = TRUE, check.names = FALSE)
nrow(RESULT)
PARAM<-1
# THE RESULTS OF THE OAT ANALYSIS IS IN ONE PLACE. THUS WE NEED TO
# REFER TO THE CORRECT BASELINE RESULT FOR PARAMETERS THAT ARE
# NOT BEING CHANGED SO WE USE THE VARIABLE EXP_PARAMS WHEN WE START
#A NEW VARIABLE - WE SET THE PARAMS TO THE BASELINE AND THEN ONLY
# ALTER THE ONE BEING CHANGED
EXP_PARAMS <- as.character(BASELINE)
# NOW GET THE LIST OF PARAMETER VALUES BEING EXPLORED FOR THIS
# PARAMETER. NOTE CONVERSION TO NUMBERS: GETS RID OF TRAILING
# ZEROS MADE BY SEQ
val_list <- as.numeric(prepare_parameter_value_list(PMIN, PMAX, PINC,
PARAMVALS, PARAM))
PARAMVALS<-NULL
# NOW GET THE LIST OF PARAMETER VALUES BEING EXPLORED FOR THIS
# PARAMETER. NOTE CONVERSION TO NUMBERS: GETS RID OF TRAILING
# ZEROS MADE BY SEQ
val_list <- as.numeric(prepare_parameter_value_list(PMIN, PMAX, PINC,
PARAMVALS, PARAM))
val_list
ALLRESULTS <- NULL
for (PARAMVAL in 1:length(val_list)) {
EXP_PARAMS[PARAM] <- as.character(val_list[PARAMVAL])
PARAM_RESULT <- subset_results_by_param_value_set(PARAMETERS,
RESULT, EXP_PARAMS)
VALUE_RESULT <- cbind(PARAM_RESULT[PARAMETERS[PARAM]])
# NOW ADD ALL MEASURES
for (MEASURE in 1:length(MEASURES)) {
VALUE_RESULT <- cbind(VALUE_RESULT,
PARAM_RESULT[MEASURES[MEASURE]])
}
ALLRESULTS <- rbind(ALLRESULTS, VALUE_RESULT)
}
ALLRESULTS
MEASURE<-1
GRAPHTITLE <- paste("Distribution of ", MEASURES[MEASURE],
" Responses \n when altering parameter ",
PARAMETERS[PARAM], sep = "")
ALLRESULTS[PARAMETERS[PARAM]] <-as.factor(as.matrix(ALLRESULTS[PARAMETERS[PARAM]]))
ALLERSULTS[PARAMETERS[PARAM]]
ALLRESULTS[PARAMETERS[PARAM]]
ALLRESULTS$quantity
ggplot2::ggplot(ALLRESULTS, ggplot2::aes(x=as.numeric(ALLRESULTS[,1]), y=ALLRESULTS[, MEASURE+1])) +
ggplot2::geom_boxplot(notch=TRUE, outlier.shape = outlier_flag) + ggplot2::labs(title=GRAPHTITLE,x="Parameter Value",
y = paste0("Median ", MEASURES[MEASURE], " (",MEASURE_SCALE[PARAM], ")")) +
ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5)) +
ggplot2::stat_summary(fun.y=median, geom="point", shape=23, size=2)
outlier_flag=TRUE
ggplot2::ggplot(ALLRESULTS, ggplot2::aes(x=as.numeric(ALLRESULTS[,1]), y=ALLRESULTS[, MEASURE+1])) +
ggplot2::geom_boxplot(notch=TRUE, outlier.shape = outlier_flag) + ggplot2::labs(title=GRAPHTITLE,x="Parameter Value",
y = paste0("Median ", MEASURES[MEASURE], " (",MEASURE_SCALE[PARAM], ")")) +
ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5)) +
ggplot2::stat_summary(fun.y=median, geom="point", shape=23, size=2)
ggplot2::ggplot(ALLRESULTS, ggplot2::aes(x=ALLRESULTS[,1], y=ALLRESULTS[, MEASURE+1])) +
ggplot2::geom_boxplot(notch=TRUE, outlier.shape = outlier_flag) + ggplot2::labs(title=GRAPHTITLE,x="Parameter Value",
y = paste0("Median ", MEASURES[MEASURE], " (",MEASURE_SCALE[PARAM], ")")) +
ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5)) +
ggplot2::stat_summary(fun.y=median, geom="point", shape=23, size=2)
MEASURE<-2
GRAPHTITLE <- paste("Distribution of ", MEASURES[MEASURE],
" Responses \n when altering parameter ",
PARAMETERS[PARAM], sep = "")
ALLRESULTS[PARAMETERS[PARAM]] <-as.factor(as.matrix(ALLRESULTS[PARAMETERS[PARAM]]))
ggplot2::ggplot(ALLRESULTS, ggplot2::aes(x=ALLRESULTS[,1], y=ALLRESULTS[, MEASURE+1])) +
ggplot2::geom_boxplot(notch=TRUE, outlier.shape = outlier_flag) + ggplot2::labs(title=GRAPHTITLE,x="Parameter Value",
y = paste0("Median ", MEASURES[MEASURE], " (",MEASURE_SCALE[PARAM], ")")) +
ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5)) +
ggplot2::stat_summary(fun.y=median, geom="point", shape=23, size=2)
head(ALLRESULTS)
ALLRESULTS$quantity
ALLRESULTS
sapply(ALLRESULTS,as.numeric)
nrow(ALLRESULTS)
ggplot2::ggplot(ALLRESULTS, ggplot2::aes(x=reorder(ALLRESULTS[,1],sort(as.numeric(ALLRESULTS[,1]))), y=ALLRESULTS[, MEASURE+1])) +
ggplot2::geom_boxplot(notch=TRUE, outlier.shape = outlier_flag) + ggplot2::labs(title=GRAPHTITLE,x="Parameter Value",
y = paste0("Median ", MEASURES[MEASURE], " (",MEASURE_SCALE[PARAM], ")")) +
ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5)) +
ggplot2::stat_summary(fun.y=median, geom="point", shape=23, size=2)
MEASURE<-1
if (is.null(TIMEPOINTS)) {
GRAPHTITLE <- paste("Distribution of ", MEASURES[MEASURE],
" Responses \n when altering parameter ",
PARAMETERS[PARAM], sep = "")
} else {
GRAPHTITLE <- paste("Distribution of ", MEASURES[MEASURE],
" Responses \n when altering parameter ",
PARAMETERS[PARAM], " at Timepoint ",
TIMEPOINTS, " ", TIMEPOINTSCALE,
sep = "")
}
GRAPHTITLE <- paste("Distribution of ", MEASURES[MEASURE],
" Responses \n when altering parameter ",
PARAMETERS[PARAM], sep = "")
ALLRESULTS[PARAMETERS[PARAM]] <-as.factor(as.matrix(ALLRESULTS[PARAMETERS[PARAM]]))
if(!outliers)
{
outlier_flag=NA
} else {
outlier_flag = 1
}
ggplot2::ggplot(ALLRESULTS, ggplot2::aes(x=reorder(ALLRESULTS[,1],sort(as.numeric(ALLRESULTS[,1]))), y=ALLRESULTS[, MEASURE+1])) +
ggplot2::geom_boxplot(notch=TRUE, outlier.shape = outlier_flag) + ggplot2::labs(title=GRAPHTITLE,x="Parameter Value",
y = paste0("Median ", MEASURES[MEASURE], " (",MEASURE_SCALE[PARAM], ")")) +
ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5)) +
ggplot2::stat_summary(fun.y=median, geom="point", shape=23, size=2)
??reorder
devtools::load_all(".")
devtools::check()
devtools::build()
devtools::check()
library(spartan)
library(ggplot2)
install.packages("ggplot2")
install.packages(c("devtools","lhs","XML"))
install.packages("lhs","plotrix")
install.packages(c("lhs","plotrix"))
install.packages(c("mlegp","ggplot2","neuralnet"))
install.packages(c("psych","mco","randomForest","e1071","XML")
)
install.packages(c("knitr","rmarkdown","testthat"))
