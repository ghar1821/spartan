sample_maxes<-range_check$sample_maxes
sample_mins
sample_maxes<-apply(experiment_results,2,max)
sample_mins<-apply(experiment_results,2,min)
length(sample_mins)
length(parameters)
length(sample_mins)==length(parameters) & length(sample_maxes)==length(parameters)
length(sample_maxes)==length(parameters)
sample_maxezx
sample_maxes
colnames(sample_maxes)
names(sample_maxes)
# If we normalise, we need to have the mins and maxes for parameters and
# measures for denormalisation of results. If we don't normalise then
# there will be no denormalisation, but the values being passed will not
# be initialised, so we need to cope with both here
pre_normed_data_mins <- NULL
pre_normed_data_maxes <- NULL
normed_data <- normalise_dataset(dataset, sample_mins, sample_maxes,
parameters)
normed_data
names(sample_mins)
sampleMins <-cbind(0,0.1,0.1,0.015,0.1,0.25)
sampleMins
names(sampleMins)
colnames(sampleMins)
names(sampleMins)<-c(4,5,6,7,8,9)
colnames(sampleMins)
devtools::load_all(".")
devtools::build()
head(experiment_results)
# Added 18th October 2018 in developing RoboSpartan - was noticed that user could
# submit dataset where whole columns were the same, which in normalisation causes
# issues. As such results have no use in the analysis, the decision was taken to
# remove these features
dataset_check<-dataset_precheck(dataset, parameters, measures)
dataset_check
is.null(colnames(sample_mins)) | is.null(colnames(sample_maxes))
is.null(colnames(sample_mins))
sample_mins
names(sample_mins)
is.null(names(sample_mins)) | is.null(names(sample_maxes))
devtools::load_all()
devtools::build()
load("/home/kja505/Documents/spartanDB/Built_Emulation_RF_SVM.Rda")
View(built_emulation)
two_emulators<-built_emulation
load("/home/kja505/Documents/spartanDB/Built_Emulation_RF.Rda")
one_emulator<-built_emulation
names(two_emulators)
names(one_emulator)
names(two_emulators$emulators
)
two_emulators$emulators
one_emulator$emulators
emulation<-two_emulators
load("~/Desktop/validationSet.Rda")
data_to_predict<-validation_set
normalise_result<-FALSE
normalise=FALSE
normalise_result<-TRUE
length(emulation$emulators)==1
predictions <- NULL
length(emulation$emulators)
e<-1
model_predictions <- generate_predictions_from_emulator(
emulation$emulators[[e]], parameters, measures, data_to_predict)
model_predictions
predictions <- cbind(
predictions, extract_predictions_from_result_list(
model_predictions, emulation$emulators[[e]]$type, measures))
predictions
e<-2
model_predictions <- generate_predictions_from_emulator(
emulation$emulators[[e]], parameters, measures, data_to_predict)
predictions <- cbind(
predictions, extract_predictions_from_result_list(
model_predictions, emulation$emulators[[e]]$type, measures))
predictions
# rbind used to get the mins and maxes into the correct format
# (compatible with all other calls to this function)
predictions <- denormalise_dataset(
predictions, rbind(emulation$pre_normed_mins[measures]),
rbind(emulation$pre_normed_maxes[measures] ))
predictions
predictions <- NULL
# Run this for all emulators in the object
for (e in 1:length(emulation$emulators)) {
model_predictions <- generate_predictions_from_emulator(
emulation$emulators[[e]], parameters, measures, data_to_predict)
predictions <- cbind(
predictions, extract_predictions_from_result_list(
model_predictions, emulation$emulators[[e]]$type, measures))
}
normalised_data<-predictions
scaled_mins<-rbind(emulation$pre_normed_mins[measures])
scaled_maxes<-rbind(emulation$pre_normed_maxes[measures] )
scaled_mins
scaled_maxes
length(emulation$emulators))
length(emulation$emulators)
predictions
emulation$emulators$type
emulation$emulators
emulation$emulators[[1]]
emulation$emulators[[1]]$type
paste0(emulation$emulators[[1]]$type,"_",measures)
# Generate the header of the predictions for this emulator so these can be extracted: emulator type "_" measure
predictions_header<-paste0(emulation$emulators[[1]]$type,"_",measures)
predictions[predictions_header]
predictions[,predictions_header]
predictions
i<-1
# Generate the header of the predictions for this emulator so these can be extracted: emulator type "_" measure
predictions_header<-paste0(emulation$emulators[[i]]$type,"_",measures)
predictions[,predictions_header] <- denormalise_dataset(
predictions[,predictions_header], rbind(emulation$pre_normed_mins[measures]),
rbind(emulation$pre_normed_maxes[measures]))
predictions
i<-2
predictions[,predictions_header] <- denormalise_dataset(
predictions[,predictions_header], rbind(emulation$pre_normed_mins[measures]),
rbind(emulation$pre_normed_maxes[measures]))
predictions
predictions <- NULL
# Run this for all emulators in the object
for (e in 1:length(emulation$emulators)) {
model_predictions <- generate_predictions_from_emulator(
emulation$emulators[[e]], parameters, measures, data_to_predict)
predictions <- cbind(
predictions, extract_predictions_from_result_list(
model_predictions, emulation$emulators[[e]]$type, measures))
}
for(i in 1:length(emulation$emulators))
{
# Generate the header of the predictions for this emulator so these can be extracted: emulator type "_" measure
predictions_header<-paste0(emulation$emulators[[i]]$type,"_",measures)
predictions[,predictions_header] <- denormalise_dataset(
predictions[,predictions_header], rbind(emulation$pre_normed_mins[measures]),
rbind(emulation$pre_normed_maxes[measures]))
}
predictions
devtools::load_all(".")
devtools::build()
devtools::load_all(".")
devtools::build()
devtools::load_all(".")
devtools::build()
load("/home/kja505/Desktop/emulator.Rda")
parameters<-c("stableBindProbability","chemokineExpressionThreshold","initialChemokineExpressionValue","maxChemokineExpressionValue","maxProbabilityOfAdhesion","adhesionFactorExpressionSlope")
measures<-c("Velocity","Displacement")
baseline<- c(50,0.3, 0.2, 0.04, 0.60, 1.0)
minvals <- c(10, 0.10, 0.10, 0.015, 0.1, 0.25)
maxvals <- c(100, 0.9, 0.50, 0.08, 0.95, 5.0)
incvals <- c(10, 0.1, 0.05, 0.005, 0.05, 0.25)
measure_scale<-c("Velocity","Displacement")
output_directory<-"/home/kja505/Desktop"
devtools::load_all(".")
emulate_lhc_sampled_parameters(output_directory, emulator, parameters, measures, measure_scale, dataset = data.frame(parameter_value_set), normalise = TRUE, write_csv_files = FALSE)
load("/home/kja505/Desktop/pvs.Rda")
emulate_lhc_sampled_parameters(output_directory, emulator, parameters, measures, measure_scale, dataset = data.frame(parameter_value_set), normalise = TRUE, write_csv_files = FALSE)
surrogateModel<-emulator
filepath<-output_directory
dataset<-data.frame(parameter_value_set)
normalise=TRUE
write_csv_files=FALSE
!is.null(param_file) | !is.null(dataset)
param_file<-NULL
!is.null(param_file) | !is.null(dataset)
!is.null(param_file)
spartan_sample <- dataset
param_values_with_predictions <- NULL
samp<-1
# Retrieve the parameter row:
params <- spartan_sample[samp, ]
ensemble_set == FALSE
ensemble_set = TRUE
prediction <- use_ensemble_to_generate_predictions(surrogateModel,
params, parameters,
measures,
normalise_values = normalise,
normalise_result = normalise)
prediction
param_values_with_predictions <- NULL
for (samp in 1:nrow(spartan_sample)) {
# Retrieve the parameter row:
params <- spartan_sample[samp, ]
# generate the prediction - need to be careful here whether we are
# using an ensemble or emulator
if(ensemble_set == FALSE) {
prediction <- emulator_predictions(surrogateModel, parameters, measures,
params, normalise)
} else {
prediction <- use_ensemble_to_generate_predictions(surrogateModel,
params, parameters,
measures,
normalise_values = normalise,
normalise_result = normalise)
}
# Bind to the list of parameters
param_values_with_predictions <- rbind(param_values_with_predictions,
cbind(params, prediction))
}
# Now write this out as if it was the spartan LHC summary
colnames(param_values_with_predictions) <- c(parameters, measures)
write_csv_files
# Analyse:
# Method takes care of adding timepoint to the file names
prccs<-lhc_generatePRCoEffs(FILEPATH=filepath, parameters,  measures,
"Emulated_LHC_Summary.csv",
"CorrelationCoefficients.csv",
c(timepoint), timepointscale, check_done=TRUE,
write_csv_files=write_csv_files)
FILEPATH<-filepath
PARAMETERS<-parameters
MEASURES<-measures
LHCSUMMARYFILENAME<-"Emulated_LHC_Summary.csv"
CORCOEFFSOUTPUTFILE<-"CorrelationCoefficients.csv"
lhc_result_file <- read_from_csv(file.path(FILEPATH,LHCSUMMARYFILENAME))
devtools::load_all(".")
devtools::load_all(".")
# Analyse:
# Method takes care of adding timepoint to the file names
prccs<-lhc_generatePRCoEffs(FILEPATH=filepath, parameters,  measures,
LHCSUMMARYFILENAME=NULL,
"CorrelationCoefficients.csv",
c(timepoint), timepointscale, check_done=TRUE,
write_csv_files=write_csv_files,
lhc_summary_object=param_values_with_predictions)
prccs
devtools::load_all(".")
devtools::load_all(".")
lhc_graphMeasuresForParameterChange(
filepath, parameters, measures, measure_scale, CORCOEFFSOUTPUTFILE=NULL,
LHCSUMMARYFILENAME=NULL, OUTPUT_TYPE = c("PNG"), TIMEPOINTS = c(timepoint),
TIMEPOINTSCALE = timepointscale, check_done=TRUE, corcoeffs_output_object=prccs,
lhc_summary_object=param_values_with_predictions)
devtools::build()
devtools::load_all(".")
devtools::build()
devtools::load_all(".")
devtools::build()
devtools::load_all(".")
devtools::build()
load("/home/kja505/Desktop/partitionedData.Rda")
emulator_list<-c("RF","SVM")
model_list<-emulator_list
parameters
measures
algorithm_settings<-NULL
algorithm_settings <- emulation_algorithm_settings()
length(parameters) != length(partitioned_data$parameters)
length(measures) != length(partitioned_data$measures)
all_model_predictions <- NULL
all_model_accuracy_stats <- NULL
# Structure to store the emulators created
ensemble_emulators <- vector("list", length(model_list))
model_index<-1
# Sse the same code to generate all the currently acceptable model types
model_fit <- generate_emulator_model(model_list[model_index],
parameters, measures,
partitioned_data,
algorithm_settings,
timepoint = timepoint, normalised,
output_formats)
technique<-model_list[model_index]
dataset<-partitioned_data
start.time <- proc.time()
models_for_all_measures <- vector("list", length(measures))
m<-1
# Make the formula
model_formula <- generate_model_formula(parameters, measures[m])
print(model_formula)
technique
# Generate the RF:
model_fit <- randomForest::randomForest(
model_formula, data = dataset$training[, c(parameters, measures[m])],
ntree = algorithm_settings$num_trees)
# Stats for the training fit:
model_training_fit <- predict(model_fit,
newdata = dataset$training[, parameters])
algorithm_settings$plot_test_accuracy == TRUE
normalised
# Want to denormalise the predictions such that these are plotted
# on the correct scale
unscaled_predictions <- denormalise_dataset(
cbind(model_training_fit),
rbind(dataset$pre_normed_mins[measures[m]]),
rbind(dataset$pre_normed_maxes[measures[m]]))
unscaled_simulations <- denormalise_dataset(
dataset$training, rbind(dataset$pre_normed_mins),
rbind(dataset$pre_normed_maxes))
normalised_data<-dataset$training
scaled_mins<-rbind(dataset$pre_normed_mins)
scaled_maxes<-rbind(dataset$pre_normed_maxes)
ncol(normalised_data)
scaled_mins
scaled_maxes
c<-1
normalised_data[, c]
normalised_data[, c] <- (normalised_data[, c] *
(scaled_maxes[c] -
scaled_mins[c])) + scaled_mins[c]
unscaled_simulations <- denormalise_dataset(
dataset$training, rbind(dataset$pre_normed_mins),
rbind(dataset$pre_normed_maxes))
devtools::load_all(".")
unscaled_simulations <- denormalise_dataset(
dataset$training, rbind(dataset$pre_normed_mins),
rbind(dataset$pre_normed_maxes))
normalised_data[,2]
normalised_data
partitioned_data
partitioned_data$training
partitioned_data$training[,c]
normalised_data
partitioned_data
partitioned_data$training
normalised_data<-dataset$training
normalised_data
ncol(normalised_data)
for (c in 1:ncol(normalised_data)) {
print(c)
normalised_data[, c] <- (normalised_data[, c] *
(scaled_maxes[c] -
scaled_mins[c])) + scaled_mins[c]
}
scaled_maxes[c]
scaled_mins[c]
scaled_maxes[c] -
scaled_mins[c]
normalised_data[, c]
c
normalised_data
normalised_data<-dataset$training
normalised_data[,1]
normalised_data[,2]
normalised_data[,3]
normalised_data[,4]
normalised_data[,5]
normalised_data[,6]
normalised_data[,7]
normalised_data[,8]
normalised_data[,9]
(normalised_data[, c] *
(scaled_maxes[c] -
scaled_mins[c])) + scaled_mins[c]
normalised_data[, c]
(scaled_maxes[c] -
scaled_mins[c])) + scaled_mins[c]
(normalised_data[, c] *
(scaled_maxes[c] -
scaled_mins[c])) + scaled_mins[c]
normalised_data<-data.frame(dataset$training)
(normalised_data[, c] *
(scaled_maxes[c] -
scaled_mins[c])) + scaled_mins[c]
scaled_mins
rbind(scaled_mins)
ncol(scaled_mins)
f<-data.frame(scaled_mins)
g<-data.frame(scaled_maxes)
normalised_data[,c]
normalised_data[,c]*scaled_maxes[c]
normalised_data[,c]*scaled_maxes[,c]
normalised_data[, c]
normalised_data
normalised_data<-data.frame(normalised_data)
normalised_data[,c]
scaled_maxes[c]
scaled_maxes[c][[1]]
(normalised_data[, c] *
(scaled_maxes[c][[1]] -
scaled_mins[c][[1]])) + scaled_mins[c][[1]]
for (c in 1:ncol(normalised_data)) {
print(c)
normalised_data[, c] <- (normalised_data[, c] *
(scaled_maxes[c][[1]] -
scaled_mins[c][[1]])) + scaled_mins[c][[1]]
}
normalised_data
devtools::load_all()
devtools::build()
devtools::test()
data("emulated_lhc_values")
download.file("www.kieranalden.info/spartan/test_data/built_ensemble_72.Rda", "built_ensemble")
# Load this in
load("built_ensemble")
dir.create(file.path(getwd(),"SA"))
emulate_lhc_sampled_parameters(file.path(getwd(),"SA"), built_ensemble,
c("stableBindProbability","chemokineExpressionThreshold",
"initialChemokineExpressionValue",
"maxChemokineExpressionValue","maxProbabilityOfAdhesion",
"adhesionFactorExpressionSlope"), c("Velocity","Displacement"),
c("microns","microns/min"), dataset = emulated_lhc_values, normalise = TRUE)
filepath<-file.path(getwd(),"SA")
surrogateModel<-built_ensemble
parameters
measures
measure_scale
dataset<-emulated_lhc_values
normalise = TRUE
spartan_sample <- dataset
param_values_with_predictions <- NULL
for (samp in 1:nrow(spartan_sample)) {
# Retrieve the parameter row:
params <- spartan_sample[samp, ]
# generate the prediction - need to be careful here whether we are
# using an ensemble or emulator
if(ensemble_set == FALSE) {
prediction <- emulator_predictions(surrogateModel, parameters, measures,
params, normalise)
} else {
prediction <- use_ensemble_to_generate_predictions(surrogateModel,
params, parameters,
measures,
normalise_values = normalise,
normalise_result = normalise)
}
# Bind to the list of parameters
param_values_with_predictions <- rbind(param_values_with_predictions,
cbind(params, prediction))
}
# Now write this out as if it was the spartan LHC summary
colnames(param_values_with_predictions) <- c(parameters, measures)
write_csv_files = TRUE
lhcsummaryfilename <- file.path(filepath, "Emulated_LHC_Summary.csv")
correlation_coeffs <- file.path(filepath, "CorrelationCoefficients.csv")
write.csv(param_values_with_predictions, lhcsummaryfilename,
quote = FALSE, row.names = FALSE)
# Analyse:
# Method takes care of adding timepoint to the file names
prccs<-lhc_generatePRCoEffs(FILEPATH=filepath, parameters,  measures,
LHCSUMMARYFILENAME=NULL,
"CorrelationCoefficients.csv",
c(timepoint), timepointscale, check_done=TRUE,
write_csv_files=write_csv_files,
lhc_summary_object=param_values_with_predictions)
graph_results
graph_results=TRUE
lhc_graphMeasuresForParameterChange(
filepath, parameters, measures, measure_scale, CORCOEFFSOUTPUTFILE=NULL,
LHCSUMMARYFILENAME=NULL, OUTPUT_TYPE = c("PNG"), TIMEPOINTS = c(timepoint),
TIMEPOINTSCALE = timepointscale, check_done=TRUE, corcoeffs_output_object=prccs,
lhc_summary_object=param_values_with_predictions)
FILEPATH<-filepath
PARAMETERS<-parameters
MEASURES<-measures
MEASURE_SCALE<-measure_scale
CORCOEFFSOUTPUTFILE=NULL
LHCSUMMARYFILENAME=NULL
OUTPUT_TYPE = c("PNG")
corcoeffs_output_object=prccs
lhc_summary_object=param_values_with_predictions
!is.null(CORCOEFFSOUTPUTFILE)
corcoeffs <- corcoeffs_output_object
lhcresult <- lhc_summary_object
message ("Generating output graphs for LHC Parameter Analysis")
p<-1
m<-1
corr_result <- corcoeffs[
p, paste(MEASURES[m], "_Estimate", sep = "")]
corr_result
corcoeffs
corcoeffs_output_object
prccs
param_values_with_predictions
ILEPATH=filepath
FILEPATH=filepath
LHCSUMMARYFILENAME=NULL
"CorrelationCoefficients.csv"
CORCOEFFSOUTPUTFILE<-"CorrelationCoefficients.csv"
cor_calc_method=c("s")
lhc_summary_object=param_values_with_predictions
!is.null(LHCSUMMARYFILENAME)
!is.null(lhc_summary_object)
lhc_result_file<-lhc_summary_object
message("Generating Partial Rank Correlation Coefficients (lhc_generatePRCoEffs)")
COEFFRESULTS <- calculate_prccs_all_parameters(PARAMETERS, lhc_result_file,
MEASURES, cor_calc_method)
COEFFRESULTS
write_csv_files
corcoeffs <- corcoeffs_output_object
corcoeffs
prccs
devtools::load_all(".")
emulate_lhc_sampled_parameters(file.path(getwd(),"SA"), built_ensemble,
c("stableBindProbability","chemokineExpressionThreshold",
"initialChemokineExpressionValue",
"maxChemokineExpressionValue","maxProbabilityOfAdhesion",
"adhesionFactorExpressionSlope"), c("Velocity","Displacement"),
c("microns","microns/min"), dataset = emulated_lhc_values, normalise = TRUE,
write_csv_files = TRUE)
devtools::load_all(".")
emulate_lhc_sampled_parameters(file.path(getwd(),"SA"), built_ensemble,
c("stableBindProbability","chemokineExpressionThreshold",
"initialChemokineExpressionValue",
"maxChemokineExpressionValue","maxProbabilityOfAdhesion",
"adhesionFactorExpressionSlope"), c("Velocity","Displacement"),
c("microns","microns/min"), dataset = emulated_lhc_values, normalise = TRUE,
write_csv_files = FALSE)
predictions<-emulate_lhc_sampled_parameters(file.path(getwd(),"SA"), built_ensemble,
c("stableBindProbability","chemokineExpressionThreshold",
"initialChemokineExpressionValue",
"maxChemokineExpressionValue","maxProbabilityOfAdhesion",
"adhesionFactorExpressionSlope"), c("Velocity","Displacement"),
c("microns","microns/min"), dataset = emulated_lhc_values, normalise = TRUE,
write_csv_files = FALSE)
file.exists(file.path(getwd(),"SA","stableBindProbability_Velocity.png"))
devtools::load_all(".")
devtools::test()
devtools::check()
